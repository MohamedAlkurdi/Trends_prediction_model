# A script generated by a LLM model to automate the fetching of Google Trends data.
# Did not work. to many 429 errors.

import pandas as pd
import time
import random
from pytrends.request import TrendReq
from datetime import datetime, timedelta

# Initialize pytrends client
pytrends = TrendReq(hl='en-US', tz=360)

# Constants
MAX_MONTHS = 8  # Google Trends daily limit
DATE_FORMAT = "%Y-%m-%d"

def chunk_dates(start_date, end_date, chunk_size_months=MAX_MONTHS):
    """Split date range into chunks of up to chunk_size_months."""
    chunks = []
    start = start_date
    while start < end_date:
        end = (start + pd.DateOffset(months=chunk_size_months)).date()
        if end > end_date:
            end = end_date
        chunks.append((start, end))
        start = end + timedelta(days=1)
    return chunks

def safe_fetch(keyword, geo, start_date, end_date, retries=10):
    """Fetch interest_over_time with retry logic."""
    timeframe = f"{start_date.strftime(DATE_FORMAT)} {end_date.strftime(DATE_FORMAT)}"
    for attempt in range(retries):
        
        try:
            pytrends.build_payload([keyword], geo=geo, timeframe=timeframe)
            data = pytrends.interest_over_time()
            if 'isPartial' in data.columns:
                data = data.drop(columns=['isPartial'])
            return data
        except Exception as e:
            wait = 2 ** attempt + random.uniform(1, 5)
            print(f"[{keyword}] Retry {attempt+1}/{retries} in {round(wait, 2)}s due to error: {e}")
            time.sleep(wait)
    print(f"[{keyword}] Failed to fetch after {retries} retries.")
    return pd.DataFrame()

def fetch_all_chunks(keyword, geo, start_date, end_date):
    """Fetch and concatenate all time chunks for a keyword."""
    chunks = chunk_dates(start_date, end_date)
    all_data = pd.DataFrame()
    for i, (start, end) in enumerate(chunks):
        print(f"Fetching chunk {i+1}/{len(chunks)}: {start} to {end}")
        df = safe_fetch(keyword, geo, start, end)
        if not df.empty:
            all_data = pd.concat([all_data, df])
        time.sleep(random.uniform(5, 10))  # prevent rate limiting
    return all_data

def main():
    # Configurations
    topics = ["Novel", "Letter", "Writing"]  # Keywords (you can also use topic IDs)
    countries = ["Canada"]
    start_date = datetime(2012, 1, 1).date()
    end_date = datetime(2017, 5, 4).date()

    for country in countries:
        for topic in topics:
            print(f"\nüîç Fetching '{topic}' for {country}...")
            df = fetch_all_chunks(topic, country, start_date, end_date)
            if not df.empty:
                df.rename(columns={topic: topic.lower().replace(" ", "_")}, inplace=True)
                df.to_csv(f"{country}_{topic.lower().replace(' ', '_')}.csv")
                print(f"‚úÖ Saved {len(df)} rows to {country}_{topic.lower().replace(' ', '_')}.csv")
            else:
                print(f"‚ö†Ô∏è No data fetched for {topic} in {country}")

def split_columns(path):
    df = pd.read_csv(path)
    columns = df.columns.tolist()
    for col in columns:
        if col != 'date':
            new_df = df[['date', col]].copy()
            new_df.to_csv(f"{col}.csv", index=False)
            print(f"{col} was saved successfully.")

if __name__ == "__main__":
    main()
    
    # split_columns("C:/Users/alkrd/Desktop/graduation_project/the_project/Classification/output/regions/north_america_australia/genral_labeled_data_with_relative_traffic_rates/Intellectualism/extended_data/usa_trend_data/regressors/Book_Literature_History_Religion_education.csv")